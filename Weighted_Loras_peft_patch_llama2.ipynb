{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"78GbyM0s21yx"},"outputs":[],"source":["!pip install transformers==4.31.0\n","!pip install bitsandbytes==0.41.0\n","!pip install accelerate==0.21.0\n","!pip install sentencepiece==0.1.99\n","!pip install peft==0.4.0"]},{"cell_type":"markdown","source":["# Load base llama-2 13b model\n","\n","*Note you will need to use a valid huggingface access token in access_token variable.*"],"metadata":{"id":"5JvvKr7o4vBX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGvxgVAI3-7k"},"outputs":[],"source":["import torch\n","from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n","from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n","\n","model_path = \"meta-llama/Llama-2-13b-hf\"\n","access_token = '***'\n","\n","tokenizer = LlamaTokenizer.from_pretrained(model_path, token=access_token)\n","tokenizer.pad_token = 0\n","tokenizer.padding_side = \"left\"\n","\n","model = LlamaForCausalLM.from_pretrained(\n","    model_path,\n","    token=access_token,\n","    device_map={\"\": 0},\n","    load_in_4bit=True,\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            llm_int8_threshold=6.0,\n","            llm_int8_has_fp16_weight=False,\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_quant_type='nf4',\n","        ),\n",")"]},{"cell_type":"markdown","source":["# Patch peft\n","\n","The following code provides patches for the peft Linear Lora layers and their quantized versions.\n","\n","Additionally, it introduces two new methods to the main model:\n","\n","*   **load_multiple_loras** : This method loads a list of Lora adapters all at once.\n","*   **set_lora_activations** : This method sets the weights of each Lora for inference (batch inference compatible).\n","\n"],"metadata":{"id":"fmOiGuYP5WTp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkaDLB_A3_jK"},"outputs":[],"source":["import peft\n","import torch.nn.functional as F\n","\n","def Linear_forward(self, x: torch.Tensor):\n","    previous_dtype = x.dtype\n","    if self.active_adapter not in self.lora_A.keys():\n","        return F.linear(x, peft.utils.transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n","    if self.disable_adapters:\n","        if self.r[self.active_adapter] > 0 and self.merged:\n","            self.unmerge()\n","        result = F.linear(x, peft.utils.transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n","    elif self.r[self.active_adapter] > 0 and not self.merged:\n","        result = F.linear(x, peft.utils.transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n","\n","        x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n","\n","        ones = torch.ones(list(x[0].shape[:-1]) + [1])\n","        mults = []\n","        for activ in list(map(list, zip(*self.lora_activation))):\n","            mults.append(torch.stack([ones*_x for _x in activ]).to(x.device))\n","\n","        outputs = []\n","        for i, adpt in enumerate(list(self.r.keys())):\n","            _x = x.to(self.lora_A[adpt].weight.dtype)\n","            outputs.append(\n","                self.lora_B[adpt](\n","                    self.lora_A[adpt](self.lora_dropout[adpt](_x))\n","                )\n","                * self.scaling[adpt]\n","            )\n","            outputs[i] = outputs[i]*mults[i]\n","\n","        result += torch.sum(torch.stack(outputs), 0)\n","    else:\n","        result = F.linear(x, peft.utils.transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n","\n","    result = result.to(previous_dtype)\n","\n","    return result\n","\n","def Linear8bitLt_forward(self, x: torch.Tensor):\n","    result = super(peft.tuners.lora.Linear8bitLt, self).forward(x)\n","\n","    ones = torch.ones(list(x[0].shape[:-1]) + [1])\n","    mults = []\n","    for activ in list(map(list, zip(*self.lora_activation))):\n","        mults.append(torch.stack([ones*_x for _x in activ]).to(x.device))\n","\n","    if self.disable_adapters or self.active_adapter not in self.lora_A.keys():\n","        return result\n","    elif self.r[self.active_adapter] > 0:\n","        if not torch.is_autocast_enabled():\n","            expected_dtype = result.dtype\n","\n","            if x.dtype != torch.float32:\n","                x = x.float()\n","            outputs = []\n","            for i, adpt in enumerate(list(self.r.keys())):\n","                _x = x.to(self.lora_A[adpt].weight.dtype)\n","                outputs.append(\n","                    self.lora_B[adpt](\n","                        self.lora_A[adpt](self.lora_dropout[adpt](_x))\n","                    ).to(expected_dtype)\n","                    * self.scaling[adpt]\n","                )\n","                outputs[i] = outputs[i]*mults[i]\n","            output = torch.sum(torch.stack(outputs), 0)\n","        else:\n","            outputs = []\n","            for i, adpt in enumerate(list(self.r.keys())):\n","                _x = x.to(self.lora_A[adpt].weight.dtype)\n","                outputs.append(\n","                    self.lora_B[adpt](\n","                        self.lora_A[adpt](self.lora_dropout[adpt](_x))\n","                    ).to(expected_dtype)\n","                    * self.scaling[adpt]\n","                )\n","                outputs[i] = outputs[i]*mults[i]\n","            output = torch.sum(torch.stack(outputs), 0)\n","        result += output\n","    return result\n","\n","def Linear4bit_forward(self, x: torch.Tensor):\n","    result = super(peft.tuners.lora.Linear4bit, self).forward(x)\n","\n","    ones = torch.ones(list(x[0].shape[:-1]) + [1])\n","    mults = []\n","    for activ in list(map(list, zip(*self.lora_activation))):\n","        mults.append(torch.stack([ones*_x for _x in activ]).to(x.device))\n","\n","    if self.disable_adapters or self.active_adapter not in self.lora_A.keys():\n","        return result\n","    elif self.r[self.active_adapter] > 0:\n","        result = result.clone()\n","        if not torch.is_autocast_enabled():\n","            expected_dtype = result.dtype\n","            outputs = []\n","            for i, adpt in enumerate(list(self.r.keys())):\n","                _x = x.to(self.lora_A[adpt].weight.dtype)\n","                outputs.append(\n","                    self.lora_B[adpt](\n","                        self.lora_A[adpt](self.lora_dropout[adpt](_x))\n","                    ).to(expected_dtype)\n","                    * self.scaling[adpt]\n","                )\n","                outputs[i] = outputs[i]*mults[i]\n","            output = torch.sum(torch.stack(outputs), 0)\n","        else:\n","            outputs = []\n","            for i, adpt in enumerate(list(self.r.keys())):\n","                outputs.append(\n","                    self.lora_B[adpt](\n","                        self.lora_A[adpt](self.lora_dropout[adpt](x))\n","                    ).to(expected_dtype)\n","                    * self.scaling[adpt]\n","                )\n","                outputs[i] = outputs[i]*mults[i]\n","            output = torch.sum(torch.stack(outputs), 0)\n","\n","        result += output\n","    return result\n","\n","def load_multiple_loras(self, lora_dict):\n","    \"\"\"Monkey patch method for LlamaForCausalLM : loads multiple lora adapters at the same time\n","\n","    Args:\n","      lora_dict (dict) : dictionnary with adapter name as key and path as values.\n","          Example : {'guanaco':'Mikael110/llama-2-13b-guanaco-qlora', 'dolphin':'dfurman/llama-2-13b-dolphin-peft'}\n","\n","    Returns: None\n","    \"\"\"\n","    for l in lora_dict:\n","        _model = peft.PeftModel.from_pretrained(\n","            self,\n","            lora_dict[l],\n","            adapter_name=l,\n","            force_download=False\n","        )\n","\n","def set_lora_activations(self, lora_activation):\n","    \"\"\"Monkey patch method for LlamaForCausalLM : sets loras activation weights to Lora layers for a given batch\n","\n","    Args:\n","      lora_activation (array): should be a 2 dimensional array\n","        - first dimension being the batch element\n","        - second dimension being the weight per lora adapter\n","        Example for 3 lora layers, batch of two : [[1st_lora_adapter_1st_element, 2nd_lora_adapter_1st_element, 3rd_lora_adapter_1st_element],\n","                                                   [1st_lora_adapter_2nd_element, 2nd_lora_adapter_2nd_element, 3rd_lora_adapter_2nd_element]]\n","\n","    Returns: None\n","    \"\"\"\n","    for module in self.model.modules():\n","        if isinstance(module, peft.tuners.lora.LoraLayer):\n","            module.lora_activation = lora_activation\n","\n","peft.tuners.lora.Linear.forward = Linear_forward\n","peft.tuners.lora.Linear8bitLt.forward = Linear8bitLt_forward\n","peft.tuners.lora.Linear4bit.forward   = Linear4bit_forward\n","LlamaForCausalLM.set_lora_activations = set_lora_activations\n","LlamaForCausalLM.load_multiple_loras  = load_multiple_loras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaBZzi-7cAUe"},"outputs":[],"source":["model.load_multiple_loras({'guanaco':'Mikael110/llama-2-13b-guanaco-qlora',\n","                           'dolphin':'dfurman/llama-2-13b-dolphin-peft',\n","                           'codealpaca':'layoric/llama-2-13b-code-alpaca-qlora'})"]},{"cell_type":"markdown","source":["# Inference\n","\n","The code below shows how to use a modified model to run batch inference with a different load weighting for each input.\n","\n","Llama-2 seems to struggle with padded inputs. You will get better results with a single input genration (either batch size 1, or same prompt used multiple times)."],"metadata":{"id":"3LiBhL1o4PFe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTHj1aDB4Fmb"},"outputs":[],"source":["# Code prompt\n","code_prompt = '### Instruction:\\nWrite python code to call a REST Webservice at path /user and POST a following json data name : Mick, lastname : jaeger, login : bigboy, password : password123.\\n\\\n","Start by explaining step by step how you will proceed, then write the code, commenting each part.\\n\\n\\\n","### Response:\\n'\n","# Orca prompt\n","orca_prompt = 'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-bystep and justify your answer.\\n\\\n","### Instruction: John was a terrible writer. To practice, his teacher suggest that he consider people he knows and do what? Options: - write novels - advertising firm - write letter - write notes - write poems Let\\'s think now! Step-by-step reasoning:\\n\\n\\\n","### Response:'\n","# Guanaco prompt\n","guanaco_prompt = '### Instruction:\\nWhat weights more, 1kg of feathers or 1kg of lead?\\n\\\n","Reason step by step and give your answer.\\n\\n\\\n","### Response:\\n'\n","\n","input_ids = tokenizer([guanaco_prompt,orca_prompt,code_prompt], return_tensors=\"pt\", padding=True).input_ids\n","lora_activation = [[1, 0, 0],\n","                   [0, 1, 0],\n","                   [0, 0, 1]]\n","model.set_lora_activations(lora_activation=lora_activation)\n","\n","generation_output = model.generate(\n","    input_ids=input_ids,\n","    max_new_tokens=128,\n","    temperature=0.01,\n","    repetition_penalty=1.1\n",")\n","\n","print(tokenizer.decode(generation_output[0]))\n","print('----')\n","print(tokenizer.decode(generation_output[1]))\n","print('----')\n","print(tokenizer.decode(generation_output[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8781rL2yNk2e"},"outputs":[],"source":["# Guanaco prompt\n","guanaco_prompt = '### Instruction:\\nWhat weights more, 1kg of feathers or 1kg of lead?\\n\\\n","Reason step by step and give your answer.\\n\\n\\\n","### Response:\\n'\n","\n","input_ids = tokenizer([guanaco_prompt,guanaco_prompt], return_tensors=\"pt\").input_ids\n","lora_activation = [[1, 0, 0],\n","                   [1, 0.75, 0]]\n","model.set_lora_activations(lora_activation=lora_activation)\n","\n","generation_output = model.generate(\n","    input_ids=input_ids,\n","    max_new_tokens=64,\n","    temperature=0.01,\n","    repetition_penalty=1.1\n",")\n","print(tokenizer.decode(generation_output[0]))\n","print('----')\n","print(tokenizer.decode(generation_output[1]))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1xfAOcN5jVL1s0xByWgmcF07Y9tTZBrlA","timestamp":1690666598872}],"authorship_tag":"ABX9TyOdvPZm56kzAxxu5gugOCPK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}